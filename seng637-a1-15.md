>   **SENG 637 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 15      |
|-----------------|
| Sadia Khan               |   
| Om V. Patel            |   
| Sanket Patel               |   
| Ramanpreet Kaur              |   


**Table of Contents**

(When you finish writing, update the following list using right click, then
“Update Field”)

[1 Introduction	1](#_Toc439194677)

[2 High-level description of the exploratory testing plan	1](#_Toc439194678)

[3 Comparison of exploratory and manual functional testing	1](#_Toc439194679)

[4 Notes and discussion of the peer reviews of defect reports	1](#_Toc439194680)

[5 How the pair testing was managed and team work/effort was
divided	1](#_Toc439194681)

[6 Difficulties encountered, challenges overcome, and lessons
learned	1](#_Toc439194682)

[7 Comments/feedback on the lab and lab document itself	1](#_Toc439194683)

# Introduction

### Exploratory Testing
Exploratory testing is a flexible and intuitive software testing approach where testers explore the application without following a predefined script. Instead of relying on a set plan, testers use their understanding of the software to guide their testing. The more they learn about the application, the better they can identify potential issues. This method allows testers to design and execute tests on the spot, adjusting their approach as they go. Since there are no strict guidelines, testers have the freedom to navigate the application in any direction to uncover bugs.

### Manual Functional/Scripted Testing
Manual scripted testing is a structured approach where testers follow a predefined script containing specific test cases and steps. This script is created in advance and must be executed exactly as written, without deviation. Because the steps are clearly documented, different testers can follow the same process. However, a limitation of this method is that if an issue falls outside the scripted test cases, it may go unnoticed every time.

# High-level description of the exploratory testing plan

The goal is to test whether the ATM functions as expected by simulating a real customer’s experience. Instead of focusing on deep testing of a few features, the plan is to briefly test all major functions to catch any obvious issues. The approach ensures that a user can successfully complete common banking tasks without errors while also checking for potential problems when pressing various buttons or taking less common actions.

### Functions Tested:
- Turning the ATM ON/OFF
- Inserting a card
- Deposit
- Withdraw
- Balance Inquiry
- Transfer Funds
- Receipt collected
- Showing/hiding/clearing the log

### Approach:
- **Go step by step** – Test functions in order to not miss anything (Deposit → Withdraw → Transfer → Balance Inquiry)
- **Test a little of everything** – Try each function briefly rather than going deep into a few
- **Think like a customer** – Use the ATM as a real user would and see if things work smoothly
- **Press everything** – Try all buttons and options to check for issues

### Test Cases:
- **Common paths** – Standard transactions (ex. deposit money, withdraw from checking, check balance)
- **Edge cases** – Try things like withdrawing more than the balance or transferring with insufficient funds
- **Basic function check** – If something works once, move on to the next test
- **Usability** – See if the process is easy to follow and messages make sense

# Comparison of exploratory and manual functional testing
In the early stages of testing, we used Exploratory Testing, where we worked in pairs. One person used the ATM app to perform various operations, while the other recorded any defects they noticed. Later, we moved on to Manual Functional Testing, where the entire group worked together to follow the test cases listed in Appendix C. In this phase, one member operated the ATM app, another read out the test case, and two members recorded defects in the backlog.

Exploratory Testing allowed us to freely navigate the app, using our knowledge to find potential issues and edge cases. It was quicker and required fewer people than Manual Functional Testing. However, it missed some defects that we later found during Manual Functional Testing. While Exploratory Testing was fast and efficient, it didn't provide complete coverage.

Manual Functional Testing, on the other hand, ensured that all major ATM functions were tested. It also documented defects thoroughly, including the steps to reproduce them and the expected vs. actual results. Though this method was more time-consuming and required more effort, it was crucial for verifying the app’s core functionality.

Both testing methods have advantages and drawbacks, making them valuable for identifying defects in any application.

# Notes and discussion of the peer reviews of defect reports
During our peer review, we divided into two pairs to review each other’s findings from exploratory and scripted tests. We discovered that each team member has a different approach to testing the ATM program. In particular, during exploratory testing, we found unique bugs with little overlap, meaning we all tested in slightly different ways.

We split the peer review into two parts: exploratory testing and scripted testing. For each part, we looked at the bugs reported by the other pair and tried to reproduce them. After reviewing, we met online to discuss our findings, focusing on any bugs that we couldn’t reproduce. This helped us learn valuable information, like how different display settings among team members led to different test results.


# How the pair testing was managed and team work/effort was divided 

During pair testing, we split into 2 pairs and agreed to return after about 30 minutes with our recorded defects. For teamwork and effort, we communicated effectively via Zoom, making it easy to collaborate and learn from each other throughout the testing process. Everyone contributed equally, asked questions when needed, and ensured the work was completed successfully and with high quality. For the lab report, we first went through each question together, noting key points and what we had learned. Then, we divided the questions among team members and later regrouped to review everyone’s answers. Before submitting, we ensured the report met all requirements.


### Team Member Role & Responsibilities

**Pair 1: Test Planning & Review**
Sadia Khan - Lead Tester & Ramanpreet Kaur - Reviewer & Validator
Identify the scope of testing and develop test plans for structured testing.
Review defect reports, reproduce issues, and verify fixes.
Test executions are performed as per predefined testing strategy.

**Pair 2: Test Execution & Documentation**
Om V. Patel - Test Executor & Sanket Patel - Documentation Specialist
Manually execute the test cases, log defects, and validate the test case results.
Create detailed defect reports, document test cases, and track issues in an orderly fashion.
Peer review findings, verify results, and improve documentation clarity among team members.


**Pair Testing Approach**
Collaborative Testing: Tests are being executed jointly by two team members; cross checking the findings is performed and results were validated.
Defect Logging & Tracking: Issues found are logged in Jira, and defects are assigned for resolution on the basis of severity.
Real-time Discussion: Any anomaly found was discussed then and there within the team for the further course of action.

Profound **Key Learning** from Pair Testing:-
Improved Accuracy & Efficiency: The paired testing concept helped in decreasing missed defects through the verification by a second tester.
Faster Defect Resolution: Because of real-time discussion, issues could be identified and removed faster, which increased the effectiveness of tests.
Improved Bug Tracking: Tracked all issues assigned for resolution in Jira, not even missing a bug.

# Difficulties encountered, challenges overcome, and lessons learned

**Difficulties Encountered & Challenges Overcome**

- There were various challenges encountered during the execution of the test cases in Appendix C for Manual Scripted Testing. For example, Test Cases 36 through 40 did not have clear initial system conditions; thus, we are not sure what would constitute the inputs for each of these test cases. This immediately showed that we could not obtain a proper result because it would not be certain what the starting state of the system should be prior to executing the tests.

- Moreover, we also knew that scripted testing generally involves a lot of heavy documentation and elaborative execution processes, which are quite time-consuming. The test scripts need to be written down in great detail, defects have to be logged with accurate information, and the progress needs to be tracked in a backlog system. It took considerable effort to do so. Second, interpreting complex test case requirements was another challenge because sometimes ambiguous or long descriptions resulted in misunderstandings and inconsistencies in test execution.

- Regardless of these issues, we got comfortable with the backlog tracking system and proficient with collaborative testing methodologies using pair testing. In particular, working in pairs improved our test accuracy through knowledge sharing, therefore enhancing early problem detection at an early process stage. Also, this whole experience highlighted for us the relevance of structured defect reporting and tracking, which stood to provide timely insight into project progress, the status of different system versions, and the solution status of particular issues.

- This lab gave us deeper knowledge of various testing methodologies, their appropriate applications, and their respective advantages and disadvantages. Knowing when to use each method allowed us to optimize our testing process and improve overall efficiency. Through this experience, we gained a comprehensive appreciation for the critical role of detailed reporting, structured tracking, and strategic test execution in ensuring software quality.


**Key Lessons Learned**

- Well-defined requirements make testing easier; well-documented requirements ensure consistency.

- Correct classification of severity contributes to efficiency: the critical ones must be dealt with first for better time management.

- Effective utilization of Jira eliminates duplicate reports: A little search before logging the defects saves efforts.

- Detail test tracking helps resolve complex issues: Capture test conditions to help debug.

- Efficiency due to familiarity with Jira: Once one gets to know ways of using Jira, one finds efficiency increasing in test management and defect tracking.

- Work is easier in collaboration: When one knows a collaboration tool, knowing the way of communication does make the work easier.
  
# Comments/feedback on the lab and lab document itself

Text…
